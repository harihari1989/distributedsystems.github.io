<!doctype html>
<html lang="en" data-color-mode="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Database Systems Internals | Distributed Systems Visual Guide</title>
    <link rel="stylesheet" href="../assets/css/theme.css" />
    <script defer src="../assets/js/toggleTheme.js"></script>
    <script defer src="../assets/js/site.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script defer src="../assets/js/mermaid-init.js"></script>
  </head>
  <body>
    <header class="site-header">
      <div class="site-brand">
        <a href="../index.html">Distributed Systems Visual Guide</a>
      </div>
      <nav class="site-nav">
        <a href="../index.html">Home</a>
        <a href="compute-patterns.html">Compute</a>
        <a href="database-architectures.html">Storage</a>
        <a href="database-patterns.html" class="is-active" aria-current="page">Databases</a>
        <a href="distributed-transactions.html">Transactions</a>
        <a href="consistency-models.html">Consistency</a>
        <a href="networking-patterns.html">Networking</a>
        <a href="kubernetes-patterns.html">Kubernetes</a>
        <a href="operating-systems.html">Operating Systems</a>
        <a href="solid-design-principles.html">SOLID Design</a>
      </nav>
      <button class="theme-toggle" type="button" onclick="toggleColorMode()">Toggle theme</button>
    </header>
    <main>
      <h1>Database Systems Internals</h1>

      <section>
        <h2>Problem framing</h2>
        <p>
          Engineers must choose data models and understand internal mechanics that shape latency, durability, and
          recovery. Write-ahead logging, storage engines, indexes, and concurrency control define the read and write
          dataflow that shows up in production. This page focuses on internals; sharding and replication mechanics live
          in <a href="database-architectures.html">storage patterns</a>.
        </p>
      </section>

      <section>
        <h2>Core idea / pattern</h2>
        <h3>Database categories</h3>
        <p>
          Problem: match the data model to access patterns without over-engineering.
          Pattern: classify workloads by model and choose the smallest category that meets the SLA.
        </p>

        <table>
          <thead>
            <tr>
              <th>Category</th>
              <th>Examples</th>
              <th>Primary use cases</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Relational</td>
              <td>PostgreSQL, MySQL</td>
              <td>Transactions, joins, strong invariants</td>
            </tr>
            <tr>
              <td>Key-value</td>
              <td>Redis, DynamoDB</td>
              <td>Caching, fast lookups</td>
            </tr>
            <tr>
              <td>Document</td>
              <td>MongoDB, CouchDB</td>
              <td>Flexible schemas, JSON data</td>
            </tr>
            <tr>
              <td>Columnar / wide-column</td>
              <td>Cassandra, Bigtable</td>
              <td>Analytics, write-heavy workloads</td>
            </tr>
            <tr>
              <td>Time-series</td>
              <td>InfluxDB, TimescaleDB</td>
              <td>Metrics, telemetry, monitoring</td>
            </tr>
            <tr>
              <td>Graph</td>
              <td>Neo4j, JanusGraph</td>
              <td>Relationship traversal, path queries</td>
            </tr>
            <tr>
              <td>Vector</td>
              <td>FAISS, Pinecone, Weaviate</td>
              <td>Semantic search, embedding retrieval</td>
            </tr>
            <tr>
              <td>Geospatial</td>
              <td>PostGIS, BigQuery GIS</td>
              <td>Spatial joins, distance queries</td>
            </tr>
          </tbody>
        </table>

        <h3>Storage engine internals</h3>
        <p>
          Problem: make writes durable while keeping read latency predictable on disk.
          Pattern: append changes to a write-ahead log, stage data in memory, and persist with B-tree or LSM layouts.
          Background compaction or vacuum reconciles on-disk structures.
        </p>

        <table>
          <thead>
            <tr>
              <th>Structure</th>
              <th>Strengths</th>
              <th>Costs</th>
              <th>Common in</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>B-tree / B+ tree</td>
              <td>Fast point and range reads</td>
              <td>Random I/O, slower sustained writes</td>
              <td>PostgreSQL, MySQL</td>
            </tr>
            <tr>
              <td>LSM tree</td>
              <td>High write throughput, sequential I/O</td>
              <td>Read amplification, compaction overhead</td>
              <td>Cassandra, RocksDB</td>
            </tr>
          </tbody>
        </table>

        <h3>Indexing techniques</h3>
        <p>
          Problem: accelerate predicates and joins without exploding write cost.
          Pattern: align index type to query shape and data distribution.
        </p>

        <table>
          <thead>
            <tr>
              <th>Index type</th>
              <th>Best for</th>
              <th>Trade-offs</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Hash</td>
              <td>Exact key lookups</td>
              <td>No range queries</td>
            </tr>
            <tr>
              <td>B+ tree</td>
              <td>Range scans, ordered queries</td>
              <td>Heavier updates</td>
            </tr>
            <tr>
              <td>Bitmap</td>
              <td>Low-cardinality filters</td>
              <td>Memory growth on high cardinality</td>
            </tr>
            <tr>
              <td>R-tree / QuadTree</td>
              <td>Spatial regions</td>
              <td>Complex updates</td>
            </tr>
            <tr>
              <td>Vector (ANN)</td>
              <td>Similarity search</td>
              <td>Approximate results, rebuild cost</td>
            </tr>
          </tbody>
        </table>

        <h3>Transactions and concurrency control</h3>
        <p>
          Problem: coordinate concurrent reads and writes without violating invariants.
          Pattern: ACID transactions implemented with locking or MVCC, tuned by isolation level
          (read committed, repeatable read, serializable). Distributed consistency trade-offs are covered in
          <a href="consistency-models.html">consistency models</a>. Cross-shard commit coordination lives in
        <a href="distributed-transactions.html">Transactions</a>
        </p>

        <h3>Query planning and execution</h3>
        <p>
          Problem: avoid pathological plans as data grows and workloads shift.
          Pattern: parse into a logical plan, optimize with statistics-driven cost models, and execute using
          iterator/volcano or vectorized engines.
        </p>
      </section>

      <section>
        <h2>Architecture diagram</h2>
        <h3>Internal components</h3>
        <pre class="mermaid">
flowchart LR
  Client[Client] --> API[Query API]
  API --> Plan[Parser + Planner]
  Plan --> Exec[Execution Engine]
  Exec --> Txn[Txn Manager]
  Txn --> Cache[Buffer Cache]
  Cache --> Storage[Storage Engine]
  Storage --> Data[Data Files]
  Txn --> WAL[Write-Ahead Log]
  WAL --> Data
  Storage --> Bg[Compaction / Vacuum]
  Txn --> Repl[Replication Log]
  Repl --> Replicas[Replicas]
        </pre>
      </section>

      <section>
        <h2>Step-by-step flow</h2>
        <ol>
          <li>The client submits a query or mutation through the database API.</li>
          <li>The parser builds a logical plan and the optimizer picks indexes and joins.</li>
          <li>The transaction manager assigns a snapshot or locks and validates constraints.</li>
          <li>Writes append to the WAL before touching in-memory pages or memtables.</li>
          <li>Reads consult the buffer cache, then hit on-disk pages or SSTables on cache miss.</li>
          <li>Background compaction or vacuum reconciles on-disk structures as results return.</li>
        </ol>
        <h3>Write path dataflow</h3>
        <pre class="mermaid">
sequenceDiagram
  participant Client
  participant API as Query API
  participant Txn as Txn Manager
  participant WAL as WAL
  participant Cache as Buffer Cache
  participant Storage as Storage Engine
  participant Disk as Data Files

  Client->>API: Write request
  API->>Txn: Begin transaction
  Txn->>WAL: Append log record
  WAL-->>Txn: Fsync and ack
  Txn->>Cache: Update pages/memtable
  Cache->>Storage: Flush when full
  Storage->>Disk: Persist pages/SSTables
  Txn-->>Client: Commit ack
        </pre>
        <h3>Read path dataflow</h3>
        <pre class="mermaid">
sequenceDiagram
  participant Client
  participant API as Query API
  participant Planner as Planner
  participant Exec as Execution Engine
  participant Cache as Buffer Cache
  participant Storage as Storage Engine
  participant Disk as Data Files

  Client->>API: Read or query
  API->>Planner: Parse and plan
  Planner->>Exec: Physical plan
  Exec->>Cache: Lookup pages/indexes
  Cache-->>Exec: Hit or miss
  Exec->>Storage: Fetch missing data
  Storage->>Disk: Read blocks
  Storage-->>Exec: Rows/kv pairs
  Exec-->>Client: Result set
        </pre>
      </section>

      <section>
        <h2>Failure modes</h2>
        <ul>
          <li>WAL fsync stalls or saturated disks cause tail latency spikes on writes.</li>
          <li>Long-running transactions hold locks or old MVCC versions and bloat storage.</li>
          <li>Compaction or vacuum storms amplify writes and slow reads.</li>
          <li>Buffer cache thrashing drives random I/O and latency variance.</li>
          <li>Bad statistics cause the optimizer to pick full scans or expensive joins.</li>
        </ul>
      </section>

      <section>
        <h2>Trade-offs</h2>
        <ul>
          <li>B-trees favor read latency and range scans; LSM trees favor write throughput.</li>
          <li>More indexes speed reads but slow writes and increase storage overhead.</li>
          <li>Higher isolation levels reduce anomalies but add contention and latency.</li>
          <li>Synchronous WAL or replication improves durability but increases write latency.</li>
          <li>Aggressive compaction or vacuum improves read speed but raises write amplification.</li>
        </ul>
      </section>

      <section>
        <h2>Real-world usage</h2>
        <p>
          Workload fit determines the primary system, while scale-out mechanics rely on
          <a href="database-architectures.html">storage patterns</a> and distributed consistency relies on
          <a href="consistency-models.html">consistency models</a>.
        </p>
        <table>
          <thead>
            <tr>
              <th>Use case</th>
              <th>Recommended database</th>
              <th>Why it fits</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>OLTP transactions</td>
              <td>PostgreSQL, MySQL</td>
              <td>ACID transactions with rich indexing</td>
            </tr>
            <tr>
              <td>Caching and fast lookup</td>
              <td>Redis, DynamoDB</td>
              <td>Low-latency key access</td>
            </tr>
            <tr>
              <td>Event ingestion</td>
              <td>Kafka, Pulsar</td>
              <td>Append-only throughput</td>
            </tr>
            <tr>
              <td>Analytics</td>
              <td>BigQuery, Snowflake</td>
              <td>Columnar execution at scale</td>
            </tr>
            <tr>
              <td>Search</td>
              <td>Elasticsearch</td>
              <td>Inverted indexes and ranking</td>
            </tr>
            <tr>
              <td>Metrics and telemetry</td>
              <td>InfluxDB, TimescaleDB, Prometheus</td>
              <td>Time partitioning and compression</td>
            </tr>
            <tr>
              <td>AI embeddings</td>
              <td>Vector databases</td>
              <td>Approximate nearest neighbor indexes</td>
            </tr>
          </tbody>
        </table>
      </section>
    </main>
    <footer class="site-footer">
      <div>Built for senior engineers and system designers.</div>
      <div>Distributed Systems Visual Guide.</div>
    </footer>
  </body>
</html>
